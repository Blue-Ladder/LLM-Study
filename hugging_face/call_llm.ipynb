{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치할 패키지\n",
    "1.  Transformers\n",
    "2. accelerate\n",
    "3. gradio\n",
    "\n",
    "``` !pip install -q transformers==4.40.0 accelerate gradio```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 module, tokenizer, model calling\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"당신은 유용한 AI 어시스던트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\"\n",
    "instruction = \"한국의 수도는 어디야?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{prompt}\"},  # 프롬프트 설정\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}  # llm에 하는 질문\n",
    "]\n",
    "\n",
    "# 토크나이저에 탬플릿 적용하는 것 \n",
    "input_ids = tokenizer.apply_chat_template( # 받은 메시지를 인코딩하는 부분\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"  # pytorch를 이용할 것이라는 의미\n",
    ").to(model.device)  # 인코딩된 부분을 모델 디바이스로 보냄\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# 모델의 출력을 받는 부분\n",
    "outputs = model.generate(\n",
    "    input_ids, # 위에 정의한 인풋 방식\n",
    "    max_new_tokens=512,  #생성할 토큰의 갯수 = 문장의 길이\n",
    "    eos_token_id=terminators, # eos 토큰을 terminators에 적용시킴\n",
    "    do_sample=True,# 샘플링 할 거야\n",
    "    temperature=1,  # 1에 가까울수록 창의적인 답변\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True)) # 디코딩하는 과정\n",
    "\n",
    "# 아래처럼 작성해도 괜찮으나 imstart와 같은 토큰이 나오게 됨\n",
    "# decoded = tokenizer.batch_decode(outputs)\n",
    "# print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
